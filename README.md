# Movies-ETL
Berkeley Bootcamp - Module 8

## Project Overview
In this module, you’ll learn how to use the Extract, Transform, Load (ETL) process to create data pipelines. A data pipeline moves data from a source to a destination, and the ETL process creates data pipelines that also transform the data along the way. Analysis is impossible without access to good data, so creating data pipelines is often the first step before any analysis can be performed. Therefore, understanding ETL is an essential skill for data analysis.

## Resources
- Software: PostgresSQL Version 12, Jupyter Notebook 6.0.1

## Challenge Overview
In this challenge, you will write a Python script that performs all three ETL steps on the Wikipedia and Kaggle data. To complete this task, you can use the code you created in your Jupyter Notebook, but don’t just copy and paste! You’ll need to leave out any code that performs exploratory data analysis, and you may need to add code to handle potentially unforeseen errors due to changes in the underlying data.

## Challenge Summary

Documented Assumptions:

1.
2.
3.
4.
5.
